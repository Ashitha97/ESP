{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Info\n",
    "\n",
    "Trying to Identify the best features to use for our algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "from library import lib_aws, lib_cleaning\n",
    "from library.lib_metrics import MultiClassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually importing data from s3\n",
    "file_path = \"s3://et-oasis/failure-excel/Oasis Complete Failure List 2018-2020_ver6.xlsx\"\n",
    "\n",
    "failures = pd.read_excel(file_path)\n",
    "# Cleaning WELL NAMES\n",
    "failures['Well'] = (failures['Well'].str.replace(\"#\", \"\")  # remove #\n",
    "                                 .str.replace('\\s+', ' ', regex=True)  # remove multiple spaces if present\n",
    "                                 .str.strip()  # Remove trailing whitespaces\n",
    "                                 .str.lower()  # lower all character\n",
    "                                 .str.title()  # Uppercase first letter of each word\n",
    "                                 .map(lambda x: x[0:-2] + x[-2:].upper()))\n",
    "\n",
    "failures.rename(columns={'Well': 'NodeID',\n",
    "                        'LAST OIL': 'Failure Start Date',\n",
    "                        'FAILURE END DATE': 'Failure End Date'}, inplace=True)\n",
    "well_list = failures.NodeID.unique().tolist()  # List of wells to use for querying the features and model training\n",
    "failures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "Import the data depending on how the model is to be trained and the use case:\n",
    "\n",
    "- For Training: Only wells which have been labelled\n",
    "- For Hisorical Predictions: The entire dataset\n",
    "- For Real Time Predictions: Will depend on how the data pipeline has been set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# for well_list use only those wells which have been labeled\n",
    "data_query = \"\"\"\n",
    "SELECT\n",
    "    \"NodeID\",\n",
    "    \"Date\",\n",
    "    \"PPRL\",\n",
    "    \"MPRL\",\n",
    "    \"FluidLoadonPump\",\n",
    "    \"PumpIntakePressure\"\n",
    "FROM xspoc.xdiag\n",
    "WHERE \"NodeID\" in {}\n",
    "ORDER BY \"NodeID\",\"Date\"\n",
    "\"\"\".format(tuple(well_list))\n",
    "\n",
    "with lib_aws.PostgresRDS(db='oasis-prod') as engine:\n",
    "    data = pd.read_sql(data_query, engine, parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>FluidLoadonPump</th>\n",
       "      <th>MPRL</th>\n",
       "      <th>NodeID</th>\n",
       "      <th>PPRL</th>\n",
       "      <th>PumpIntakePressure</th>\n",
       "      <th>Components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-06-21 15:58:34</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>16811.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27639.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-06-21 16:25:36</td>\n",
       "      <td>3241.0</td>\n",
       "      <td>16752.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27457.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-06-21 18:25:16</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>16594.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27448.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-06-21 18:28:10</td>\n",
       "      <td>3327.0</td>\n",
       "      <td>16595.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27424.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-06-21 20:25:01</td>\n",
       "      <td>3341.0</td>\n",
       "      <td>16711.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27662.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  FluidLoadonPump     MPRL        NodeID     PPRL  \\\n",
       "0 2019-06-21 15:58:34           3280.0  16811.0  Aagvik 1-35H  27639.0   \n",
       "1 2019-06-21 16:25:36           3241.0  16752.0  Aagvik 1-35H  27457.0   \n",
       "2 2019-06-21 18:25:16           3330.0  16594.0  Aagvik 1-35H  27448.0   \n",
       "3 2019-06-21 18:28:10           3327.0  16595.0  Aagvik 1-35H  27424.0   \n",
       "4 2019-06-21 20:25:01           3341.0  16711.0  Aagvik 1-35H  27662.0   \n",
       "\n",
       "   PumpIntakePressure Components  \n",
       "0                 NaN     Normal  \n",
       "1                 NaN     Normal  \n",
       "2                 NaN     Normal  \n",
       "3                 NaN     Normal  \n",
       "4                 NaN     Normal  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure info in these in these wells\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NodeID</th>\n",
       "      <th>Failure Start Date</th>\n",
       "      <th>LOE START DATE</th>\n",
       "      <th>LOE FINISH DATE</th>\n",
       "      <th>Failure End Date</th>\n",
       "      <th>Components</th>\n",
       "      <th>Job Type</th>\n",
       "      <th>Job Bucket</th>\n",
       "      <th>Primary Symptom</th>\n",
       "      <th>Secondary Symptom</th>\n",
       "      <th>Root Cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bouvardia Federal 2658 12-12H</td>\n",
       "      <td>2019-08-02 09:57:11</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>2019-08-24 08:31:02</td>\n",
       "      <td>BHA - Seat Nipple</td>\n",
       "      <td>1-1/2\" PUMP</td>\n",
       "      <td>PUMP</td>\n",
       "      <td>Pump Unseating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Erosion Abrasion - Fluids, Solids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Johnson 29-30H</td>\n",
       "      <td>2019-07-08 12:36:03</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>2019-07-16 21:27:22</td>\n",
       "      <td>BHA - Seat Nipple</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>BHA</td>\n",
       "      <td>Erosion - Fluid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undetermined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Didrick 4X-27H</td>\n",
       "      <td>2020-01-30 11:53:33</td>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>2020-02-15 10:16:06</td>\n",
       "      <td>BHA - TAC</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>BHA</td>\n",
       "      <td>Corrosion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Improper Chemical Usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Johnsrud 5198 14-18 15TX</td>\n",
       "      <td>2019-08-29 04:00:28</td>\n",
       "      <td>2019-09-05</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>2019-09-14 07:01:19</td>\n",
       "      <td>BHA - TAC</td>\n",
       "      <td>BHA - TAC</td>\n",
       "      <td>TUBING</td>\n",
       "      <td>Mechanically Induced Damage</td>\n",
       "      <td>Compression</td>\n",
       "      <td>System Design Needs Improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lundeen 4-26H</td>\n",
       "      <td>2019-07-29 12:32:23</td>\n",
       "      <td>2019-08-04</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>2019-08-13 09:11:02</td>\n",
       "      <td>BHA - TAC</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>BHA</td>\n",
       "      <td>Mechanically Induced Damage</td>\n",
       "      <td>Corrosion</td>\n",
       "      <td>Rod Design Needs Improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>Rolfson N 5198 12-17 5T</td>\n",
       "      <td>2019-10-24 13:11:57</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>2019-11-07 07:45:00</td>\n",
       "      <td>Tubing Leak</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>TUBING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mechanically Induced Damage</td>\n",
       "      <td>Tubing Leak - No Hole Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>Rj Titus 6093 42-20H</td>\n",
       "      <td>2019-09-13 13:33:45</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>2019-09-29 08:38:56</td>\n",
       "      <td>Tubing Leak</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>TUBING</td>\n",
       "      <td>Compression</td>\n",
       "      <td>Corrosion</td>\n",
       "      <td>Tubing Leak - No Hole Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>Hendricks 5602 43-36 2T</td>\n",
       "      <td>2019-08-11 13:49:43</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>2019-08-26 08:59:47</td>\n",
       "      <td>Tubing Leak</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>TUBING</td>\n",
       "      <td>Corrosion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tubing Leak - No Hole Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>Crawford 5493 44-7T</td>\n",
       "      <td>2019-07-28 12:24:42</td>\n",
       "      <td>2019-07-30</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019-08-01 15:14:46</td>\n",
       "      <td>Tubing Leak</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>TUBING</td>\n",
       "      <td>Corrosion</td>\n",
       "      <td>Flumping</td>\n",
       "      <td>Tubing Leak - No Hole Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>Mccauley 5501 14-3 2B</td>\n",
       "      <td>2019-07-08 14:01:13</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>2019-07-18 00:00:00</td>\n",
       "      <td>Tubing Leak</td>\n",
       "      <td>TUBING LEAK</td>\n",
       "      <td>TUBING</td>\n",
       "      <td>Corrosion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tubing Leak - No Hole Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            NodeID   Failure Start Date LOE START DATE  \\\n",
       "0    Bouvardia Federal 2658 12-12H  2019-08-02 09:57:11     2019-08-13   \n",
       "1                   Johnson 29-30H  2019-07-08 12:36:03     2019-07-12   \n",
       "2                   Didrick 4X-27H  2020-01-30 11:53:33     2020-02-10   \n",
       "3         Johnsrud 5198 14-18 15TX  2019-08-29 04:00:28     2019-09-05   \n",
       "4                    Lundeen 4-26H  2019-07-29 12:32:23     2019-08-04   \n",
       "..                             ...                  ...            ...   \n",
       "237        Rolfson N 5198 12-17 5T  2019-10-24 13:11:57     2019-10-28   \n",
       "238           Rj Titus 6093 42-20H  2019-09-13 13:33:45     2019-09-25   \n",
       "239        Hendricks 5602 43-36 2T  2019-08-11 13:49:43     2019-08-19   \n",
       "240            Crawford 5493 44-7T  2019-07-28 12:24:42     2019-07-30   \n",
       "241          Mccauley 5501 14-3 2B  2019-07-08 14:01:13     2019-07-12   \n",
       "\n",
       "    LOE FINISH DATE    Failure End Date         Components     Job Type  \\\n",
       "0        2019-08-19 2019-08-24 08:31:02  BHA - Seat Nipple  1-1/2\" PUMP   \n",
       "1        2019-07-16 2019-07-16 21:27:22  BHA - Seat Nipple  TUBING LEAK   \n",
       "2        2020-02-14 2020-02-15 10:16:06          BHA - TAC  TUBING LEAK   \n",
       "3        2019-09-11 2019-09-14 07:01:19          BHA - TAC    BHA - TAC   \n",
       "4        2019-08-12 2019-08-13 09:11:02          BHA - TAC  TUBING LEAK   \n",
       "..              ...                 ...                ...          ...   \n",
       "237      2019-11-04 2019-11-07 07:45:00        Tubing Leak  TUBING LEAK   \n",
       "238      2019-09-27 2019-09-29 08:38:56        Tubing Leak  TUBING LEAK   \n",
       "239      2019-08-22 2019-08-26 08:59:47        Tubing Leak  TUBING LEAK   \n",
       "240      2019-08-01 2019-08-01 15:14:46        Tubing Leak  TUBING LEAK   \n",
       "241      2019-07-17 2019-07-18 00:00:00        Tubing Leak  TUBING LEAK   \n",
       "\n",
       "    Job Bucket              Primary Symptom            Secondary Symptom  \\\n",
       "0         PUMP               Pump Unseating                          NaN   \n",
       "1          BHA              Erosion - Fluid                          NaN   \n",
       "2          BHA                    Corrosion                          NaN   \n",
       "3       TUBING  Mechanically Induced Damage                  Compression   \n",
       "4          BHA  Mechanically Induced Damage                    Corrosion   \n",
       "..         ...                          ...                          ...   \n",
       "237     TUBING                          NaN  Mechanically Induced Damage   \n",
       "238     TUBING                  Compression                    Corrosion   \n",
       "239     TUBING                    Corrosion                          NaN   \n",
       "240     TUBING                    Corrosion                     Flumping   \n",
       "241     TUBING                    Corrosion                          NaN   \n",
       "\n",
       "                            Root Cause  \n",
       "0    Erosion Abrasion - Fluids, Solids  \n",
       "1                         Undetermined  \n",
       "2              Improper Chemical Usage  \n",
       "3      System Design Needs Improvement  \n",
       "4         Rod Design Needs Improvement  \n",
       "..                                 ...  \n",
       "237        Tubing Leak - No Hole Found  \n",
       "238        Tubing Leak - No Hole Found  \n",
       "239        Tubing Leak - No Hole Found  \n",
       "240        Tubing Leak - No Hole Found  \n",
       "241        Tubing Leak - No Hole Found  \n",
       "\n",
       "[242 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure Label Distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tubing - Body                48\n",
       "Pump - Plunger               43\n",
       "Rod - Main Body              25\n",
       "Pump - Stuck Pump            22\n",
       "Polish Rod                   19\n",
       "Tubing Leak                  17\n",
       "Rod - Pin                    12\n",
       "Pump - Junked                 9\n",
       "Rod - 6\" Critical Section     8\n",
       "Pump - Barrel                 8\n",
       "Rod - Coupling                7\n",
       "Pump - No-Tap                 6\n",
       "Tubing - Collar               5\n",
       "Pump - On - Off Tool          4\n",
       "BHA - TAC                     3\n",
       "BHA - Seat Nipple             2\n",
       "Pump - Standing Valve         2\n",
       "Pump - Traveling Valve        1\n",
       "Pump - Valve Rod              1\n",
       "Name: Components, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Failur Info only from wells present in the data\n",
    "failure_info = failures[failures.NodeID.isin(well_list_features)]\n",
    "failure_info.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# info\n",
    "display(data.head())\n",
    "print(\"Failure info in these in these wells\")\n",
    "display(failure_info)\n",
    "print(\"Failure Label Distribution\")\n",
    "display(failure_info.Components.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before analysing the data we need to merge the information\n",
    "Transfering info from failures to data (copy of features)\n",
    "Using a for loop -- may not be very efficient\n",
    "\"\"\"\n",
    "\n",
    "def fill_null(df, chk_col='PPRL', well_col='NodeID', time_col='Date'):\n",
    "    \"\"\"\n",
    "    This function will fill in Null Values on those dates where no datapoints are present\n",
    "    Helps Show failures where no data was present\n",
    "    Will have to take this into account when running analysis \n",
    "    \"\"\"\n",
    "    data_temp = df.copy()\n",
    "    # Set time col as index if it is not\n",
    "    if time_col in data_temp.columns:\n",
    "        data_temp.set_index(time_col, inplace=True)\n",
    "    \n",
    "    data_gp = data_temp.groupby(well_col).resample('1D').max()  # Groupby wellname and resample to Day freq\n",
    "    data_gp.drop(columns=[well_col], inplace=True)  # Drop these columns as they are present in the index\n",
    "    data_gp.reset_index(inplace=True)  # Get Back WellCol from\n",
    "    data_null = data_gp[data_gp.loc[:, chk_col].isnull()]  # Get all null values, which need to be added to the main data file\n",
    "    data_null.reset_index(inplace=True, drop=True)\n",
    "    data_temp.reset_index(inplace=True)  # get timestamp back in the column for concating\n",
    "    data_full = pd.concat([data_temp, data_null], axis=0, ignore_index=True)  # concat null and og files\n",
    "    data_full.sort_values(by=[well_col, time_col], inplace=True)\n",
    "    data_full.drop_duplicates(subset=[well_col, time_col], inplace=True)\n",
    "    data_full.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data_full\n",
    "\n",
    "# TODO: transfer_cols only works for multiple columns, get it to work with 1 column\n",
    "def failure_merge(df, failure_df, transfer_cols):\n",
    "    \"\"\"\n",
    "    Merges the failures info\n",
    "    :param df: dataframe to which info is being transferred to. (Should have columns \"NodeID\" and \"Date\")\n",
    "    :param failure_df: Failure info data (Should have columns \"NodeID\", \"Start Date\" and \"End Data\")\n",
    "    :param cols: Columns which need to be transferred\n",
    "    \"\"\"\n",
    "    merged = df.copy()  \n",
    "    for col in transfer_cols:\n",
    "        merged[col] = 'Normal'  # for now putting everything as normal (even NAN's)\n",
    "        \n",
    "    for i in failure_df.index:\n",
    "        well = failure_df.loc[i, 'NodeID']\n",
    "        t_start = failure_df.loc[i, 'Failure Start Date']\n",
    "        t_end = failure_df.loc[i, 'Failure End Date'] + pd.Timedelta('1 day')  # As we have day based frequency (the times in a day are considered as 00:00:00)\n",
    "        bool_ = (merged.NodeID == well) & (merged.Date >= t_start) & (merged.Date <= t_end)  # Boolean mask for main data\n",
    "        merged.loc[bool_, transfer_cols] = failure_df.loc[i, transfer_cols].values\n",
    "        \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 41s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Components</th>\n",
       "      <th>Date</th>\n",
       "      <th>FluidLoadonPump</th>\n",
       "      <th>MPRL</th>\n",
       "      <th>NodeID</th>\n",
       "      <th>PPRL</th>\n",
       "      <th>PumpIntakePressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2019-06-21 15:58:34</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>16811.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27639.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2019-06-21 16:25:36</td>\n",
       "      <td>3241.0</td>\n",
       "      <td>16752.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27457.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2019-06-21 18:25:16</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>16594.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27448.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2019-06-21 18:28:10</td>\n",
       "      <td>3327.0</td>\n",
       "      <td>16595.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27424.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2019-06-21 20:25:01</td>\n",
       "      <td>3341.0</td>\n",
       "      <td>16711.0</td>\n",
       "      <td>Aagvik 1-35H</td>\n",
       "      <td>27662.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Components                Date  FluidLoadonPump     MPRL        NodeID  \\\n",
       "0     Normal 2019-06-21 15:58:34           3280.0  16811.0  Aagvik 1-35H   \n",
       "1     Normal 2019-06-21 16:25:36           3241.0  16752.0  Aagvik 1-35H   \n",
       "2     Normal 2019-06-21 18:25:16           3330.0  16594.0  Aagvik 1-35H   \n",
       "3     Normal 2019-06-21 18:28:10           3327.0  16595.0  Aagvik 1-35H   \n",
       "4     Normal 2019-06-21 20:25:01           3341.0  16711.0  Aagvik 1-35H   \n",
       "\n",
       "      PPRL  PumpIntakePressure  \n",
       "0  27639.0                 NaN  \n",
       "1  27457.0                 NaN  \n",
       "2  27448.0                 NaN  \n",
       "3  27424.0                 NaN  \n",
       "4  27662.0                 NaN  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data = fill_null(data)  # FIlling in Nan's where data was missing\n",
    "\n",
    "# Transfer 'Job Bucket' from failure_info to fill_data\n",
    "transfer_col = ['Components', 'Failure Start Date']\n",
    "data = failure_merge(data, failure_info, transfer_col)\n",
    "data.drop(columns='Failure Start Date', inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engg\n",
    "\n",
    "Depending on What features we and labels we want to use for our model, we can use the functions\n",
    "\n",
    "`get_agg()`:\n",
    "\n",
    "    - For now only gives us moving averages\n",
    "    - Can modify it to give other aggregate functions\n",
    "    \n",
    "`create_prediction_zones()`:\n",
    "    \n",
    "    - Will create new classes depending on what windows we choose for failures\n",
    "  \n",
    "**Note: Both these fucntions will give out separate dataframes/series and will have to be merged accordingly**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "def get_agg(df, freq, time_col='Date', well_col = 'NodeID'):\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    for well in df[well_col].unique():\n",
    "        temp_df = df[df[well_col] == well].copy()\n",
    "        temp_df.set_index(time_col, inplace=True)\n",
    "        temp_df = temp_df.rolling(freq).mean()\n",
    "        temp_df = temp_df.add_prefix(freq+'_')\n",
    "        temp_df[well_col] = well\n",
    "        temp_df.reset_index(inplace=True)\n",
    "        frames.append(temp_df)\n",
    "        \n",
    "    rolled_df = pd.concat(frames)\n",
    "    rolled_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return rolled_df\n",
    "\n",
    "\n",
    "def create_prediction_zones(df, fail_col, prediction_zone_dict):\n",
    "    \"\"\"\n",
    "    Depending on the prediction_zone_dict will create predictions zones for failures \n",
    "    in the Failure column.\n",
    "    :param df: The dataframe to extract it from\n",
    "    :param fail_col: Failure column to use from the dataframe\n",
    "    :param prediction_zone_dict: A dict with timedeltas for each type of Failure in fail_col\n",
    "    :return Will return a Series or an Array of these Prediction Zones\n",
    "    \"\"\"\n",
    "    \n",
    "    test_data = df[['NodeID', 'Date', fail_col]].copy()\n",
    "    fail_zones = test_data[fail_col]  # fail_zones will be initialized as a copy of the fail col\n",
    "    \n",
    "    # Getting start of predictions from fail col\n",
    "    fail_dates = test_data[test_data[fail_col] != 'Normal']  # everthing other than normal is considered as a prediction\n",
    "    fail_start = fail_dates[fail_dates.Date.diff().abs().fillna(pd.Timedelta('10D')) > pd.Timedelta('1d 12H')]\n",
    "    fail_start.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Adding zones by iterating over each prediction start date\n",
    "    for i in fail_start.index:\n",
    "        temp_well = fail_start.loc[i, 'NodeID']  # well name\n",
    "        zone_end_date = fail_start.loc[i, 'Date']  # prediction start date\n",
    "        fail = fail_start.loc[i, fail_col]  # actual prediction class\n",
    "        zone_delta = pd.Timedelta(prediction_zone_dict[fail])  # delta to subtract from the dictionary\n",
    "        zone_start_date = zone_end_date - zone_delta\n",
    "\n",
    "        bool_ = (test_data.NodeID == temp_well) & (test_data.Date < zone_end_date) & (test_data.Date >= zone_start_date)\n",
    "        fail_zones[bool_] = 'fz_' + fail\n",
    "        \n",
    "    return fail_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to use rolling averages with a frequency of 7 days for our features and a constant 10 day window for our failures. Follow the next few sections to see how it will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 7 day rolling averages\n",
    "avg_data = get_agg(df=data, freq='15D')  \n",
    "\n",
    "# Merge it with the original data \n",
    "# and use only those columns which will be of use\n",
    "# While working with large datasets try optmizing the copies of dataframes you create\n",
    "# May not even have to merge it\n",
    "\n",
    "full_data = data.set_index(['NodeID', 'Date']).merge(avg_data.set_index(['NodeID', 'Date']), \n",
    "                                                     left_index=True,\n",
    "                                                     right_index=True).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns that we dont need\n",
    "cols_drop = [\n",
    "    'PPRL',\n",
    "    'MPRL',\n",
    "    'PumpIntakePressure',\n",
    "    'FluidLoadonPump',\n",
    "]\n",
    "\n",
    "\n",
    "full_data.drop(columns=cols_drop, inplace=True)\n",
    "\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.isnull().sum(axis=0)/len(full_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred zone dict\n",
    "# manual\n",
    "# pred_zone_dict = {\n",
    "#     'PUMP': '15 days',\n",
    "#     'ROD': '15 days',\n",
    "#     'TUBING': '15 days',\n",
    "#     'BHA': '15 days'\n",
    "# }\n",
    "\n",
    "# automated all failures will have the same failure window\n",
    "fail_window = '15days'\n",
    "fail_labels = full_data.Components.unique().tolist()\n",
    "fail_labels.remove('Normal')\n",
    "pred_zone_dict = {x: fail_window for x in fail_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pred windows\n",
    "# Note:  The output of the fucntion will be a pandas Series\n",
    "full_data['Label'] = create_prediction_zones(df=full_data, \n",
    "                                             fail_col='Components', \n",
    "                                             prediction_zone_dict=pred_zone_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Assumtions we are going to make:\n",
    "- Drop Nan values\n",
    "- Remove the actual failures as classes and only use windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.dropna(inplace=True)\n",
    "\n",
    "class_drop = fail_labels  # just for now dont need to use it\n",
    "full_data = full_data[~full_data.Label.isin(class_drop)]\n",
    "full_data.Label = full_data.Label.str.replace('fz_', '').str.strip()\n",
    "full_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "fdt = full_data.copy()\n",
    "fdt.drop(['NodeID','Date','Components'],axis=1,inplace=True)\n",
    "\n",
    "fdt['Label'] = fdt[['Label']].apply(LabelEncoder().fit_transform)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(fdt.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysis:\n",
    "  def __init__(self,a,b):\n",
    "    self.x = a\n",
    "    self.Y = b\n",
    "\n",
    "  def e_tree(self):\n",
    "    X = self.x #independent columns\n",
    "    y = self.Y    #target column \n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(X,y)\n",
    "    print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "    #plot graph of feature importances for better visualization\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    feat_importances.nlargest(13).plot(kind='barh')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = analysis(fdt.drop('Label',axis=1),fdt['Label'])\n",
    "an.e_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data.drop(['NodeID', 'Date', 'Components','Label'],axis=1)\n",
    "Y = full_data.Label\n",
    "\n",
    "print(\"Features\")\n",
    "display(X.head())\n",
    "\n",
    "print(\"Classes Being Predicted\")\n",
    "display(Y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 1\n",
    "Random Forest classifier\n",
    "\"\"\"\n",
    "\n",
    "def build_rfc_model ():\n",
    "    \"\"\"\n",
    "    Define A Random Forrest Classifier Model\n",
    "    :return: RFC Model\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler() # Define Scaler\n",
    " \n",
    "    # RFC Params\n",
    "    rfc_params = {\n",
    "        'n_estimators': 100,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'class_weight': 'balanced',\n",
    "        'verbose': 0,\n",
    "        'max_features': 'auto',\n",
    "        'max_depth': None,\n",
    "    }\n",
    "    \n",
    "    # RFC Classifier\n",
    "    rfc = RandomForestClassifier(**rfc_params)\n",
    "    \n",
    "    #\n",
    "    model = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('rfc', rfc)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "######################\n",
    "rfc_model = build_rfc_model()\n",
    "MultiClassMetrics.baseline_metrics(X, Y, rfc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_rfc = MultiClassMetrics.cv_validation(X, Y, rfc_model)\n",
    "# print(\"CV Metrics\")\n",
    "# display(cv_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_rfc = MultiClassMetrics.kfold_validation(X, Y, rfc_model)\n",
    "print(\"Kfold Metrics\")\n",
    "display(kf_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 7\n",
    "ExtraTreesClassifier\n",
    "\"\"\"\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "def build_et_model():\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    et_params ={ 'class_weight': 'balanced', \n",
    "                'verbose': 1\n",
    "    }\n",
    "\n",
    "    et = ExtraTreesClassifier(**et_params)\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('et', et)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "###########################################\n",
    "et_model = build_et_model()\n",
    "MultiClassMetrics.baseline_metrics(X, Y, et_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_rfc = MultiClassMetrics.kfold_validation(X, Y, et_model)\n",
    "print(\"Kfold Metrics\")\n",
    "display(kf_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 6\n",
    "XGBOOST\n",
    "\"\"\"\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def build_xg_model():\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    xg_params ={ 'objective': 'multi:softmax', #multi:softprob\n",
    "                'num_class': full_data['Components'].nunique()\n",
    "    }\n",
    "\n",
    "    xg = XGBClassifier(**xg_params)\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('xg', xg)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "xg_model = build_xg_model()\n",
    "MultiClassMetrics.baseline_metrics(X, Y, xg_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_rfc = MultiClassMetrics.kfold_validation(X, Y, xg_model)\n",
    "print(\"Kfold Metrics\")\n",
    "display(kf_rfc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
