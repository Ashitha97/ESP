{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For setting up local imports in an Ipython Shell\n",
    "This is a workaround for ipython, dont need it for basic python scripts\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from library.lib_aws import AddData\n",
    "from library.lib_dyna import CardFunctions\n",
    "from config import username, password, endpoint, data_path\n",
    "import boto3\n",
    "\n",
    "# PLotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Options\n",
    "mpl.rcParams['figure.figsize'] = (15,5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "plt.style.use('dark_background')\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     12,
     16,
     26,
     71
    ]
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy.orm import Session\n",
    "# from config import username, password, endpoint\n",
    "from io import StringIO \n",
    "import csv\n",
    "import time\n",
    "from geoalchemy2 import Geometry\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from geoalchemy2.shape import from_shape\n",
    "from pyefd import elliptic_fourier_descriptors\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.wkb import loads\n",
    "import struct\n",
    "\n",
    "# Helps with relative imports from outside\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Clean up strings\n",
    "def node_clean(node_str):\n",
    "    \"\"\"\n",
    "    Function that cleans up NodeID strings\n",
    "    \"\"\"\n",
    "    node_str = \" \".join(node_str.split())  # remove empty white spaces\n",
    "    node_str = node_str.replace('#', \"\").strip().lower().title()  # remove # character, plus clean characters\n",
    "    node_str =  node_str[0:-2] + node_str[-2:].upper() # last 2 characters whill alwsy be upper case\n",
    "    return node_str\n",
    "\n",
    "# DataBase Classes\n",
    "class PostgresRDS(object):\n",
    "    \"\"\"\n",
    "    Class Connects to a PostgreSQL DB with password access\n",
    "    Need to input the database that needs to be connected to\n",
    "    Note Set the username, password and endpoint in the config file via env variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db, verbose=0):\n",
    "        self.engine = None\n",
    "        self.Session = None\n",
    "        self.db = db\n",
    "        self.vprint = print if verbose != 0 else lambda *a, **k: None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"\n",
    "        Connects to the db and gives us the engine\n",
    "        :return: engine\n",
    "        \"\"\"\n",
    "        engine_config = {\n",
    "            'sqlalchemy.url': 'postgresql+psycopg2://{user}:{pw}@{host}/{db}'.format(\n",
    "                user=username,\n",
    "                pw=password,\n",
    "                host=endpoint,\n",
    "                db=self.db\n",
    "            ),\n",
    "            'sqlalchemy.pool_pre_ping': True,\n",
    "            'sqlalchemy.pool_recycle': 3600\n",
    "        }\n",
    "\n",
    "        engine = sqlalchemy.engine_from_config(engine_config, prefix='sqlalchemy.')\n",
    "        self.Session = Session(engine)\n",
    "\n",
    "        return engine\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.engine = self.connect()\n",
    "        self.vprint(\"Connected to {} DataBase\".format(self.db))\n",
    "        return self.engine\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.Session.close()\n",
    "        self.engine.dispose()\n",
    "        self.vprint(\"Connection Closed\")\n",
    "\n",
    "\n",
    "class AddData:\n",
    "    \"\"\"\n",
    "    Class which has methods to add data into a postgres db\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def psql_insert_copy(table, conn, keys, data_iter):\n",
    "        \"\"\"\n",
    "        Execute SQL statement inserting data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        table : pandas.io.sql.SQLTable\n",
    "        conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "        keys : list of str\n",
    "            Column names\n",
    "        data_iter : Iterable that iterates the values to be inserted\n",
    "        \"\"\"\n",
    "        # gets a DBAPI connection that can provide a cursor\n",
    "        dbapi_conn = conn.connection\n",
    "        with dbapi_conn.cursor() as cur:\n",
    "            s_buf = StringIO()\n",
    "            writer = csv.writer(s_buf)\n",
    "            writer.writerows(data_iter)\n",
    "            s_buf.seek(0)\n",
    "\n",
    "            columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "            if table.schema:\n",
    "                table_name = '{}.{}'.format(table.schema, table.name)\n",
    "            else:\n",
    "                table_name = table.name\n",
    "\n",
    "            sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "                table_name, columns)\n",
    "            cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_data(df, db, table, schema=None, merge_type='append', card_col=None, index_col=None):\n",
    "        \"\"\"\n",
    "        Method to add data to a postgres db\n",
    "        :param df: Data in the form of a pandas DataFrame\n",
    "        :param db: Database Name (str)\n",
    "        :param table: Table Name (str)\n",
    "        :param schema: Schema Name (Default is None, in this case will add to the public schema)\n",
    "        :param merge_type: How to add data. Either 'append' or 'replace'. Default: 'append'\n",
    "        :param card_col: If Card Columns are present. Default('None')\n",
    "        :param index_col: If an index column is needed. Default('None')\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        if card_col is not None:\n",
    "            dtype_dict = {i: Geometry(\"POLYGON\") for i in card_col}\n",
    "        else:\n",
    "            dtype_dict = None\n",
    "\n",
    "        if index_col is not None:\n",
    "            try:\n",
    "                df.set_index(index_col, inplace=True)\n",
    "            except KeyError:  # Index Column is already set\n",
    "                pass\n",
    "\n",
    "        with PostgresRDS(db=db) as engine:\n",
    "            try:\n",
    "                df.to_sql(table, con=engine, schema=schema, if_exists=merge_type, method=AddData.psql_insert_copy,\n",
    "                          dtype=dtype_dict)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Data Not Added\")\n",
    "                return False\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(\"Data {}ed on Table {} in time {:.2f}s\".format(merge_type, table, t1 - t0))\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking how the hourly Imports look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## XDIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_name = 'XDiagResults.E2E.20200912.1900.csv'\n",
    "full_path = os.path.join(data_path, file_name)\n",
    "\n",
    "temp_xdiag = pd.read_csv(full_path, error_bad_lines=False, parse_dates=['Date', 'AnalysisDate'])\n",
    "\n",
    "# Basic Well Name Cleaning\n",
    "temp_xdiag[\"NodeID\"] = (temp_xdiag[\"NodeID\"].str.replace(\"#\", \"\")  # remove #\n",
    "                                       .str.replace('\\s+', ' ', regex=True)  # remove multiple spaces if present\n",
    "                                       .str.strip()  # Remove trailing whitespaces\n",
    "                                       .str.lower()  # lower all character\n",
    "                                       .str.title()  # Uppercase first letter of each word\n",
    "                                       .map(lambda x: x[0:-2] + x[-2:].upper()))  # last 2 characters should always be upper case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Timestamp cleaning\n",
    "max_time = pd.Timestamp(file_name.split('.')[2] + file_name.split('.')[3])\n",
    "min_time = max_time - pd.Timedelta('1 hour')\n",
    "\n",
    "print(f'For file: {file_name}\\nWe need data between {min_time} and {max_time}\\nHowever what we have is:')\n",
    "\n",
    "# OG timestamps\n",
    "data_dist = temp_xdiag.groupby([\"NodeID\"]).agg({\"AnalysisDate\": [min, max, \"count\"]})\n",
    "\n",
    "display(data_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we clean it with timestamps this is what we get\n",
    "bool_ = (temp_xdiag.AnalysisDate >= min_time) & (temp_xdiag.AnalysisDate <= max_time)\n",
    "temp_xdiag.loc[bool_, ['NodeID', 'AnalysisDate', 'Date', 'PPRL', 'MPRL', 'FluidLoadonPump', 'PumpIntakePressure']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Xdiag Rod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_name = 'XDiagRodResults.E2E.20200815.1500.csv'\n",
    "full_path = os.path.join(data_path, file_name)\n",
    "\n",
    "temp_xr = pd.read_csv(full_path, error_bad_lines=False, parse_dates=['Date'])\n",
    "temp_xr.NodeID = temp_xr.NodeID.apply(node_clean)\n",
    "\n",
    "# Timestamp cleaning\n",
    "max_time = pd.Timestamp(file_name.split('.')[2] + file_name.split('.')[3])\n",
    "min_time = max_time - pd.Timedelta('1 hour')\n",
    "\n",
    "temp_xr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bool_ = (temp_xr.Date <= max_time) \n",
    "# & (temp_xr.Date >= min_time)\n",
    "temp_xr[bool_].sort_values(by=['Date'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Well Tests\n",
    "\n",
    "- Data is coming in once everyday. \n",
    "- For a specific day (d) we get data from d-1 and d-2.\n",
    "- The datapoints from d-2 will be dropped as they will be duplicates which had already come the previous dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_name = 'WellTests.E2E.20200813.1300.csv'\n",
    "full_path = os.path.join(data_path, file_name)\n",
    "\n",
    "temp_welltests = pd.read_csv(full_path, error_bad_lines=False, parse_dates=['TestDate'])\n",
    "import_dt = file_name.split('.')[2]\n",
    "import_dt = pd.Timestamp(import_dt) - pd.Timedelta('1 day')\n",
    "# Basic Well Name Cleaning\n",
    "# temp_welltests[\"NodeID\"] = (temp_welltests[\"NodeID\"].str.replace(\"#\", \"\")  # remove #\n",
    "#                                        .str.replace('\\s+', ' ', regex=True)  # remove multiple spaces if present\n",
    "#                                        .str.strip()  # Remove trailing whitespaces\n",
    "#                                        .str.lower()  # lower all character\n",
    "#                                        .str.title()  # Uppercase first letter of each word\n",
    "#                                        .map(lambda x: x[0:-2] + x[-2:].upper()))  # last 2 characters should always be upper case\n",
    "\n",
    "temp_welltests = temp_welltests[temp_welltests.TestDate == import_dt]\n",
    "temp_welltests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OG timestamps\n",
    "# data_dist = temp_welltests.groupby([\"NodeID\"]).agg({\"TestDate\": [min, max, \"count\"]})\n",
    "\n",
    "# display(data_dist)\n",
    "\n",
    "t2 = temp_welltests[temp_welltests.TestDate == '2020-08-12'].copy()\n",
    "t2.sort_values(by=['NodeID'], inplace=True)\n",
    "t2.drop_duplicates(subset=['NodeID', 'TestDate'])\n",
    "t2.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'CardData.E2E.20200816.1200.csv'\n",
    "full_path = os.path.join(data_path, file_name)\n",
    "\n",
    "temp_card = pd.read_csv(full_path, error_bad_lines=False, parse_dates=['Date'])\n",
    "temp_card.NodeID = temp_card.NodeID.apply(node_clean)\n",
    "\n",
    "# Timestamp cleaning\n",
    "max_time = pd.Timestamp(file_name.split('.')[2] + file_name.split('.')[3])\n",
    "min_time = max_time - pd.Timedelta('1 hour')\n",
    "bool_ = (temp_card.Date <= max_time) & (temp_card.Date >= min_time)\n",
    "temp_card = temp_card[bool_]\n",
    "\n",
    "# Basic sort and dropping duplicates\n",
    "temp_card.drop_duplicates(subset=['NodeID', 'Date'], inplace=True)\n",
    "temp_card.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "temp_card.reset_index(inplace=True, drop=True)\n",
    "temp_card.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_card[temp_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## WELLTESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test out different conditions\n",
    "1. Finding out a min_time condition to remove repeatable values\n",
    "\"\"\"\n",
    "local_file_path = r'C:\\Users\\rai_v\\OneDrive\\Python Coursera\\local-data\\oasis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# XDIAG Rod\n",
    "str_match = \"XDiagRod\"\n",
    "frames = []\n",
    "for filename in os.listdir(local_file_path):\n",
    "    if str_match in filename:\n",
    "        path = os.path.join(local_file_path, filename)\n",
    "        temp_df = pd.read_csv(path, error_bad_lines=False)\n",
    "        temp_df.loc[:, 'Date'] = pd.to_datetime(temp_df.loc[:, 'Date'])\n",
    "        max_time = pd.Timestamp(filename.split('.')[2] + filename.split('.')[3])\n",
    "        temp_df = temp_df[temp_df['Date'] <= max_time]\n",
    "        temp_df['ImportDate'] = max_time\n",
    "        frames.append(temp_df)\n",
    "\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Selecting only RodNum 1 and dropping some columns for making the data set smaller\n",
    "\"\"\"\n",
    "data_cut = data[data.RodNum == 1].copy()\n",
    "data_cut.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "data_cut.reset_index(inplace=True, drop=True)\n",
    "data_cut = data_cut[['NodeID', 'Date', 'ImportDate']]\n",
    "# data_cut.drop(columns = ['Grade', 'Length', 'Diameter', 'RodGuideID', 'DragFrictionCoefficient', 'GuideCountPerRod'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_cut.NodeID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_cut['Time Diff'] = data_cut.ImportDate - data_cut.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_cut[data_cut.NodeID == \"ACADIA 31-25H\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tdiff = data_cut['Time Diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = tdiff[tdiff <= pd.Timedelta('3 days')]\n",
    "\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_cut.to_csv(\"new_xdiagrod_sampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# XdiagRod\n",
    "file1 = 'XDiagRodResults.E2E.20200815.2300.csv'\n",
    "\n",
    "tmax1 = pd.Timestamp(file1.split('.')[2] + file1.split('.')[3])\n",
    "xrod1 = pd.read_csv(os.path.join(local_file_path, file1), parse_dates=['Date'])\n",
    "xrod1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file2 = 'XDiagRodResults.E2E.20200816.0000.csv'\n",
    "\n",
    "tmax2 = pd.Timestamp(file2.split('.')[2] + file2.split('.')[3])\n",
    "xrod2 = pd.read_csv(os.path.join(local_file_path, file2), parse_dates=['Date'])\n",
    "xrod2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xrod1.sort_values(by=['Date'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xrod2[xrod2.Date <= tmax2].sort_values(by=['Date', \"NodeID\", \"RodNum\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2,
     32
    ]
   },
   "outputs": [],
   "source": [
    "# Helper fucntions\n",
    "# Dyna card functions\n",
    "def get_dyna(card_arr):\n",
    "    \"\"\"\n",
    "    Transforms Hexadecimal Dyna Card Value into Position and Load value\n",
    "    :param card_arr: Hexadecimal Array\n",
    "    :return: Position, Load 2D array\n",
    "    \"\"\"\n",
    "    if pd.isnull(card_arr):\n",
    "        pos = [0, 0, 0]\n",
    "        load = [0, 0, 0]\n",
    "\n",
    "    else:\n",
    "        test_card = card_arr.strip()\n",
    "        mid = len(test_card) / 2\n",
    "        mid = int(mid)\n",
    "\n",
    "        load = []\n",
    "        pos = []\n",
    "\n",
    "        for i in range(0, mid, 8):\n",
    "            load_temp = test_card[i:i + 8]\n",
    "            load_int = struct.unpack('f', bytes.fromhex(load_temp))[0]\n",
    "            load.append(load_int)\n",
    "\n",
    "            pos_temp = test_card[mid + i:mid + i + 8]\n",
    "            pos_int = struct.unpack('f', bytes.fromhex(pos_temp))[0]\n",
    "            pos.append(pos_int)\n",
    "\n",
    "    return np.column_stack(([pos, load]))\n",
    "\n",
    "\n",
    "def hex_to_wkb(card_arr):\n",
    "    \"\"\"\n",
    "    Transforms the Hexadecimal based card into a WKB element\n",
    "    Helps store the data in a postgis db\n",
    "    :param card_arr: Hexadecimal Card Value\n",
    "    :return: WKB card value\n",
    "    \"\"\"\n",
    "    xy = CardFunctions.get_dyna(card_arr)\n",
    "\n",
    "    try:\n",
    "        polygon = Polygon(xy)\n",
    "        wkb_element = from_shape(polygon)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        wkb_element = np.nan\n",
    "\n",
    "    return wkb_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     102,
     117
    ]
   },
   "outputs": [],
   "source": [
    "db = 'oasis-dev'\n",
    "schema = 'stream'\n",
    "\n",
    "def node_clean(node_str):\n",
    "    \"\"\"\n",
    "    Function that cleans up NodeID strings\n",
    "    \"\"\"\n",
    "    node_str = \" \".join(node_str.split())  # remove empty white spaces\n",
    "    node_str = node_str.replace('#', \"\").strip().lower().title()  # remove # character, plus clean characters\n",
    "    node_str =  node_str[0:-2] + node_str[-2:].upper() # last 2 characters whill alwsy be upper case\n",
    "    return node_str\n",
    "\n",
    "\n",
    "class OasisStream:\n",
    "\n",
    "    def __init__(self, file_path=data_path):\n",
    "        self.file_path = file_path  # Location of sftp files default to env variable \n",
    "        \n",
    "        self.str_match = 'None' \n",
    "        self.files = []\n",
    "        self.add_success = False\n",
    "        self.transfer_success = False\n",
    "        self.del_success = False\n",
    "    \n",
    "    def import_well_tests(self, table_name):\n",
    "        \"\"\"\n",
    "        This function will import welltests into a pandas df and then upload it into a PostgresDB\n",
    "        :param table_name: The table in postgres where we need to add the data\n",
    "        \"\"\"\n",
    "        self.str_match = 'WellTests'\n",
    "        self.files = []  # If runnnig imports empty files array\n",
    "        self.add_success = False  # if this file is to be run, reinitialize this variable\n",
    "        frames = []\n",
    "        for filename in os.listdir(self.file_path):\n",
    "            if self.str_match in filename:\n",
    "                self.files.append(filename)  # add filename to files array\n",
    "                path = os.path.join(self.file_path, filename)  # full location of file to import\n",
    "                temp_df = pd.read_csv(path, error_bad_lines=False, parse_dates=['TestDate'])\n",
    "                import_dt = file_name.split('.')[2]  # get the day files were generated\n",
    "                import_dt = pd.Timestamp(import_dt) - pd.Timedelta('1 day')  # this is the timestamp we need from the file\n",
    "                temp_df = temp_df[temp_df.TestDate == import_dt]\n",
    "                temp_df.NodeID = temp_df.NodeID.apply(node_clean)  # clean up NodeID strings\n",
    "                temp_df.drop_duplicates(subset=['NodeID', 'TestDate'], inplace=True)\n",
    "                frames.append(temp_df)\n",
    "        try:\n",
    "            data = pd.concat(frames)\n",
    "        except ValueError:\n",
    "            return print(\"WellTests file not present\")\n",
    "        \n",
    "        data.drop_duplicates(subset=['NodeID', 'TestDate'], inplace=True)\n",
    "        data.sort_values(by=['NodeID', 'TestDate'], inplace=True)\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        self.add_success = AddData.add_data(df=data, db=db, schema=schema,\n",
    "                                            table=table_name, merge_type='replace',\n",
    "                                            index_col='TestDate')     \n",
    "    \n",
    "    def card_data(self, table_name, cols, card_cols):\n",
    "        \"\"\"\n",
    "        Imports card data into table_name in Postgres\n",
    "        \"\"\"\n",
    "        self.str_match = 'CardData'\n",
    "        self.files = []\n",
    "        self.add_success = False\n",
    "        \n",
    "        frames = []\n",
    "        for filename in os.listdir(self.file_path):\n",
    "            if self.str_match in filename:\n",
    "                self.files.append(filename)\n",
    "                path = os.path.join(self.file_path, filename)\n",
    "                temp_df = pd.read_csv(path, error_bad_lines=False, parse_dates=['Date', 'AnalysisDate'])  # import\n",
    "                temp_df = temp_df[cols]\n",
    "                temp_df.NodeID = temp_df.NodeID.apply(node_clean) # clean nodeid\n",
    "                \n",
    "                # use only the correct timestamps\n",
    "                max_dt = pd.Timestamp(filename.split('.')[2] + filename.split('.')[3])  # Get Upper limit of time\n",
    "                min_dt = max_dt - pd.Timedelta('1 hour')\n",
    "                bool_ = (temp_df.Date <= max_dt) & (temp_df.Date >= min_dt)\n",
    "                temp_df = temp_df[bool_]\n",
    "                temp_df.drop_duplicates(subset=['NodeID', 'Date'], inplace=True)\n",
    "                frames.append(temp_df)\n",
    "        try:\n",
    "            data = pd.concat(frames)\n",
    "        except ValueError:\n",
    "            return print(\"CardData files not present\")\n",
    "        \n",
    "        data.drop_duplicates(subset=['NodeID', 'Date'], inplace=True)\n",
    "        data.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # Modifying card columns\n",
    "        data.fillna(np.nan, inplace=True)\n",
    "        try:\n",
    "            for col in card_cols:  # Converting the hex columns to a wkb format\n",
    "                data.loc[:, col] = data.loc[:, col].apply(hex_to_wkb)\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "\n",
    "        self.add_success = AddData.add_data(df=data, db=db, schema=schema,\n",
    "                                            table=table_name, merge_type='replace', card_col=card_cols,\n",
    "                                            index_col='Date') \n",
    "            \n",
    "    def transfer_s3(self, location):\n",
    "        \"\"\"\n",
    "        Transfer raw csv files to s3\n",
    "        :param location: Location in s3 bucket of csv files\n",
    "        \"\"\"\n",
    "        s3 = boto3.resource('s3')\n",
    "\n",
    "        if self.add_success is False:\n",
    "            return print(\"Data hasn't been added to RDS DB\")\n",
    "\n",
    "        for filename in self.files:\n",
    "            s3.meta.client.upload_file(os.path.join(self.file_path, filename),\n",
    "                                       \"et-oasis\", location + filename)\n",
    "        self.transfer_success = True\n",
    "\n",
    "    def del_files(self):\n",
    "\n",
    "        if self.transfer_success is False:\n",
    "            return print(\"First Transfer the files\")\n",
    "        if self.add_success is False:\n",
    "            return print(\"Add Data to db first\")\n",
    "        \n",
    "        try:\n",
    "            for filename in self.files:\n",
    "                os.remove(os.path.join(self.file_path, filename))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return print(\"Files have already been transfered\")\n",
    "\n",
    "        self.del_success = True\n",
    "        \n",
    "        return None\n",
    "            \n",
    "    def result(self):\n",
    "        print('Files Worked on:',*self.files, sep='\\n')\n",
    "        print(f'Data added to DB                    : {self.add_success}')\n",
    "        print(f'Files Transfered to S3              : {self.transfer_success}')\n",
    "        print(f'Files deleted from {self.file_path} : {self.del_success}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     7,
     25
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WellTests file not present\n",
      "Data hasn't been added to RDS DB\n",
      "First Transfer the files\n",
      "Files Worked on:\n",
      "Data added to DB                    : False\n",
      "Files Transfered to S3              : False\n",
      "Files deleted from C:\\Users\\rai_v\\OneDrive\\Python Coursera\\local-data\\oasis\\sftp-files : False\n",
      "Data replaceed on Table card in time 69.25s\n",
      "Files Worked on:\n",
      "CardData.E2E.20200817.0100.csv\n",
      "CardData.E2E.20200817.0200.csv\n",
      "CardData.E2E.20200817.0300.csv\n",
      "CardData.E2E.20200817.0400.csv\n",
      "CardData.E2E.20200817.0500.csv\n",
      "CardData.E2E.20200817.0600.csv\n",
      "CardData.E2E.20200817.0700.csv\n",
      "CardData.E2E.20200817.0800.csv\n",
      "CardData.E2E.20200817.0900.csv\n",
      "CardData.E2E.20200817.1000.csv\n",
      "CardData.E2E.20200817.1100.csv\n",
      "CardData.E2E.20200817.1200.csv\n",
      "CardData.E2E.20200817.1300.csv\n",
      "CardData.E2E.20200817.1400.csv\n",
      "CardData.E2E.20200817.1500.csv\n",
      "CardData.E2E.20200817.1600.csv\n",
      "CardData.E2E.20200817.1700.csv\n",
      "CardData.E2E.20200817.1800.csv\n",
      "CardData.E2E.20200817.1900.csv\n",
      "CardData.E2E.20200817.2000.csv\n",
      "CardData.E2E.20200817.2100.csv\n",
      "CardData.E2E.20200817.2200.csv\n",
      "CardData.E2E.20200817.2300.csv\n",
      "CardData.E2E.20200818.0000.csv\n",
      "CardData.E2E.20200818.0100.csv\n",
      "CardData.E2E.20200818.0200.csv\n",
      "CardData.E2E.20200818.0300.csv\n",
      "CardData.E2E.20200818.0400.csv\n",
      "CardData.E2E.20200818.0500.csv\n",
      "CardData.E2E.20200818.0600.csv\n",
      "CardData.E2E.20200818.0700.csv\n",
      "CardData.E2E.20200818.0800.csv\n",
      "CardData.E2E.20200818.0900.csv\n",
      "CardData.E2E.20200818.1000.csv\n",
      "CardData.E2E.20200818.1100.csv\n",
      "CardData.E2E.20200818.1200.csv\n",
      "CardData.E2E.20200818.1300.csv\n",
      "CardData.E2E.20200818.1400.csv\n",
      "CardData.E2E.20200818.1500.csv\n",
      "CardData.E2E.20200818.1600.csv\n",
      "CardData.E2E.20200818.1700.csv\n",
      "CardData.E2E.20200818.1800.csv\n",
      "CardData.E2E.20200818.1900.csv\n",
      "CardData.E2E.20200818.2000.csv\n",
      "CardData.E2E.20200818.2100.csv\n",
      "CardData.E2E.20200818.2200.csv\n",
      "CardData.E2E.20200818.2300.csv\n",
      "CardData.E2E.20200819.0000.csv\n",
      "CardData.E2E.20200819.0100.csv\n",
      "CardData.E2E.20200819.0200.csv\n",
      "CardData.E2E.20200819.0300.csv\n",
      "CardData.E2E.20200819.0400.csv\n",
      "CardData.E2E.20200819.0500.csv\n",
      "CardData.E2E.20200819.0600.csv\n",
      "CardData.E2E.20200819.0700.csv\n",
      "CardData.E2E.20200819.0800.csv\n",
      "CardData.E2E.20200819.0900.csv\n",
      "CardData.E2E.20200819.1000.csv\n",
      "CardData.E2E.20200819.1100.csv\n",
      "CardData.E2E.20200819.1200.csv\n",
      "CardData.E2E.20200819.1300.csv\n",
      "CardData.E2E.20200819.1400.csv\n",
      "CardData.E2E.20200819.1500.csv\n",
      "CardData.E2E.20200819.1600.csv\n",
      "CardData.E2E.20200819.1700.csv\n",
      "CardData.E2E.20200819.1800.csv\n",
      "CardData.E2E.20200819.1900.csv\n",
      "CardData.E2E.20200819.2000.csv\n",
      "CardData.E2E.20200819.2100.csv\n",
      "CardData.E2E.20200819.2200.csv\n",
      "CardData.E2E.20200819.2300.csv\n",
      "Data added to DB                    : True\n",
      "Files Transfered to S3              : True\n",
      "Files deleted from C:\\Users\\rai_v\\OneDrive\\Python Coursera\\local-data\\oasis\\sftp-files : False\n"
     ]
    }
   ],
   "source": [
    "stream = OasisStream()\n",
    "# Well Tests\n",
    "stream.import_well_tests(table_name='well_tests')\n",
    "stream.transfer_s3(location=\"backup/wellTests/\")\n",
    "stream.del_files()\n",
    "stream.result()\n",
    "\n",
    "# card\n",
    "cols_to_keep = [\n",
    "    \"NodeID\",\n",
    "    \"Date\",\n",
    "    \"AnalysisDate\",\n",
    "    \"SPM\",\n",
    "    \"StrokeLength\",\n",
    "    \"Runtime\",\n",
    "    \"FillBasePct\",\n",
    "    \"Fillage\",\n",
    "    \"SecondaryPumpFillage\",\n",
    "    \"POCDownholeCardB\",\n",
    "    \"SurfaceCardB\",\n",
    "    \"DownholeCardB\",\n",
    "    \"PredictedCardB\",\n",
    "    \"TorquePlotMinEnergyB\",\n",
    "    \"TorquePlotMinTorqueB\",\n",
    "    \"PermissibleLoadUpB\"\n",
    "]\n",
    "card_columns = [\n",
    "    'POCDownholeCardB',\n",
    "    'SurfaceCardB',\n",
    "    'DownholeCardB',\n",
    "    'PredictedCardB',\n",
    "    'TorquePlotMinEnergyB',\n",
    "    'TorquePlotMinTorqueB',\n",
    "    'PermissibleLoadUpB'\n",
    "]\n",
    "stream.card_data(table_name='card', cols=cols_to_keep, card_cols=card_columns)\n",
    "stream.transfer_s3(location=\"backup/card/\")\n",
    "stream.result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XDIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiagrod = OasisStream(str_match='XDiagRod', table_name='xdiagrod_test')\n",
    "xdiagrod.add_to_db()\n",
    "xdiagrod.transfer_s3(location=\"backup/xdiagRodResults/\")\n",
    "xdiagrod.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_cols_drop = [\n",
    "    'PumpCond1',\n",
    "    'PumpCond2',\n",
    "    'MonthlyElecCost',\n",
    "    'MinEnergyElecBO',\n",
    "    'MinTorqueElecBO',\n",
    "    'CurrentElecBO',\n",
    "    'AvgDHDSLoad',\n",
    "    'AvgDHUSLoad',\n",
    "    'AvgDHDSPOLoad',\n",
    "    'AvgDHUSPOLoad',\n",
    "    'DownholeAnalysisLocale',\n",
    "    'RodAnalysisLocale',\n",
    "    'SurfaceAnalysisLocale',\n",
    "    'InputAnalysisLocale'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiagres = OasisStream(str_match='XDiagResults', table_name='xdiagresults_test', drop_cols=xr_cols_drop)\n",
    "xdiagres.add_to_db()\n",
    "xdiagres.transfer_s3(location=\"backup/xdiagresults/\")\n",
    "xdiagres.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welltest = OasisStream(str_match='WellTests', table_name='welltest_test', date_col='TestDate')\n",
    "welltest.add_to_db()\n",
    "welltest.transfer_s3(location=\"backup/wellTests/\")\n",
    "# welltest.del_files()\n",
    "welltest.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     30
    ]
   },
   "outputs": [],
   "source": [
    "def get_dyna(card_arr):\n",
    "    \"\"\"\n",
    "    Transforms Hexadecimal Dyna Card Value into Position and Load value\n",
    "    :param card_arr: Hexadecimal Array\n",
    "    :return: Position, Load 2D array\n",
    "    \"\"\"\n",
    "    if pd.isnull(card_arr):\n",
    "        pos = [0, 0, 0]\n",
    "        load = [0, 0, 0]\n",
    "\n",
    "    else:\n",
    "        test_card = card_arr.strip()\n",
    "        mid = len(test_card) / 2\n",
    "        mid = int(mid)\n",
    "\n",
    "        load = []\n",
    "        pos = []\n",
    "\n",
    "        for i in range(0, mid, 8):\n",
    "            load_temp = test_card[i:i + 8]\n",
    "            load_int = struct.unpack('f', bytes.fromhex(load_temp))[0]\n",
    "            load.append(load_int)\n",
    "\n",
    "            pos_temp = test_card[mid + i:mid + i + 8]\n",
    "            pos_int = struct.unpack('f', bytes.fromhex(pos_temp))[0]\n",
    "            pos.append(pos_int)\n",
    "\n",
    "    return np.column_stack(([pos, load]))\n",
    "\n",
    "\n",
    "def hex_to_wkb(card_arr):\n",
    "    \"\"\"\n",
    "    Transforms the Hexadecimal based card into a WKB element\n",
    "    Helps store the data in a postgis db\n",
    "    :param card_arr: Hexadecimal Card Value\n",
    "    :return: WKB card value\n",
    "    \"\"\"\n",
    "    xy = CardFunctions.get_dyna(card_arr)\n",
    "\n",
    "    try:\n",
    "        polygon = Polygon(xy)\n",
    "        wkb_element = from_shape(polygon)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        wkb_element = np.nan\n",
    "\n",
    "    return wkb_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backed up Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from geoalchemy2.shape import from_shape\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.wkb import loads\n",
    "from pyefd import elliptic_fourier_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path = r'C:\\Users\\rai_v\\OneDrive\\Python Coursera\\local-data\\oasis'\n",
    "file_name = 'CardData.E2E.20200728.1100.csv'\n",
    "time_max = pd.Timestamp(file_name.split('.')[2] + file_name.split('.')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# global \n",
    "cols_to_drop = [\n",
    "    'SurfaceCard',\n",
    "    'DownholeCard',\n",
    "    'PredictedCard',\n",
    "    'PocDHCard',\n",
    "    'CorrectedCard',\n",
    "    'TorquePlotMinEnergy',\n",
    "    'TorquePlotMinTorque',\n",
    "    'TorquePlotCurrent',\n",
    "    'POCDownholeCard',\n",
    "    'ElectrogramCardB'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = pd.read_csv(os.path.join(local_file_path, file_name), parse_dates=['Date'], usecols=['NodeID', 'Date', 'POCDownholeCardB', 'SurfaceCardB'])\n",
    "# data.drop(columns=cols_to_drop, inplace=True)\n",
    "data = data[data.Date <= time_max]  # Drop points which are greater than the import timestamp\n",
    "data.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.round(data.isnull().sum(axis=0)/len(data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test \n",
    "\"\"\"\n",
    "# Convert columns\n",
    "data_test = data.loc[0:10].copy()\n",
    "\n",
    "card_cols = [\n",
    "    'SurfaceCardB',\n",
    "    'POCDownholeCardB'\n",
    "]\n",
    "\n",
    "for c in card_cols:\n",
    "    data_test.loc[:,c] = data_test.loc[:,c].apply(hex_to_wkb)\n",
    "    \n",
    "display(data_test.head())\n",
    "\n",
    "# Adding data to db\n",
    "AddData.add_data(df=data_test, db='oasis-dev', table='testcards', schema='stream', merge_type='replace', card_col=card_cols, index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For entire data\n",
    "\"\"\"\n",
    "card_cols = [\n",
    "    'SurfaceCardB',\n",
    "    'POCDownholeCardB'\n",
    "]\n",
    "\n",
    "for c in card_cols:\n",
    "    print(c)\n",
    "    data.loc[:, c] = data.loc[:, c].apply(hex_to_wkb)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddData.add_data(df=data, db='oasis-dev', table='testcards', schema='stream', merge_type='replace', card_col=card_cols, index_col='Date') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "files = []\n",
    "for filename in os.listdir(data_path):\n",
    "    if 'WellTests' in filename:\n",
    "        files.append(filename)\n",
    "        path = os.path.join(data_path, filename)\n",
    "        print(path)\n",
    "        temp_df = pd.read_csv(path, error_bad_lines=False)\n",
    "        frames.append(temp_df)\n",
    "\n",
    "try:\n",
    "    df = pd.concat(frames)\n",
    "except ValueError:\n",
    "    print(\"Files were not Present\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XDiagRod\n",
    "\n",
    "We have 2 file to use\n",
    "- XDiagRodResults.E2E.20200728.1123  --> Which has data from May 25th to July 28th\n",
    "- XDiagRodResults.E2E.20200810.1503  --> Which has data from July 28th to Aug 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xdiagrod Results\n",
    "file_path = r\"C:\\Users\\rai_v\\OneDrive\\Python Coursera\\local-data\\oasis\\back\"\n",
    "\n",
    "file1 = 'XDiagRodResults.E2E.20200810.1503.csv'\n",
    "tmax1 = pd.Timestamp(file1.split('.')[2] + file1.split('.')[3])\n",
    "\n",
    "file2 = 'XDiagRodResults.E2E.20200728.1123.csv'\n",
    "tmax2 = pd.Timestamp(file2.split('.')[2] + file2.split('.')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiag1 = pd.read_csv(os.path.join(file_path, file1), error_bad_lines=False, parse_dates=['Date'])\n",
    "print(f\"# initial: {len(xdiag1)}\")\n",
    "xdiag1 = xdiag1[xdiag1.Date <= tmax1]\n",
    "print(f\"# removing bad dates: {len(xdiag1)}\")\n",
    "xdiag1.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "xdiag1.drop_duplicates(subset=['NodeID', 'Date', 'RodNum'], inplace=True)\n",
    "print(f\"# removing duplicates: {len(xdiag1)}\")\n",
    "xdiag1.reset_index(inplace=True, drop=True)\n",
    "xdiag1.groupby(\"NodeID\").agg({\"Date\": [min, max, \"count\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiag2 = pd.read_csv(os.path.join(file_path, file2), error_bad_lines=False, parse_dates=['Date'])\n",
    "print(f\"# initial: {len(xdiag2)}\")\n",
    "xdiag2 = xdiag2[xdiag2.Date <= tmax2]\n",
    "print(f\"# removing bad dates: {len(xdiag2)}\")\n",
    "xdiag2.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "xdiag2.drop_duplicates(subset=['NodeID', 'Date', 'RodNum'], inplace=True)\n",
    "print(f\"# removing duplicates: {len(xdiag2)}\")\n",
    "xdiag2.reset_index(inplace=True, drop=True)\n",
    "xdiag2.groupby(\"NodeID\").agg({\"Date\": [min, max, \"count\"]})['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data\n",
    "xdiagrod = pd.concat([xdiag2, xdiag1])\n",
    "xdiagrod.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "print(f'Size is {len(xdiagrod)}')\n",
    "xdiagrod.drop_duplicates(subset=['NodeID', 'Date', 'RodNum'], inplace=True)\n",
    "print(f'Size is {len(xdiagrod)}')\n",
    "xdiagrod.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiagrod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddData.add_data(df=xdiagrod, db='oasis-dev', table='xdiagrod', schema='stream', merge_type='replace', card_col=None, index_col='Date') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XDiagResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xdiag Results\n",
    "file_path = r\"C:\\Users\\rai_v\\OneDrive\\Python Coursera\\local-data\\oasis\\back\"\n",
    "\n",
    "file1 = 'XDiagResults.E2E.20200728.1116.csv'\n",
    "tmax1 = pd.Timestamp(file1.split('.')[2] + file1.split('.')[3])\n",
    "\n",
    "file2 = 'XDiagResults.E2E.20200810.1500.csv'\n",
    "tmax2 = pd.Timestamp(file2.split('.')[2] + file2.split('.')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiag1 = pd.read_csv(os.path.join(file_path, file1), error_bad_lines=False, parse_dates=['Date'])\n",
    "print(f\"# initial: {len(xdiag1)}\")\n",
    "xdiag1 = xdiag1[xdiag1.Date <= tmax1]\n",
    "print(f\"# removing bad dates: {len(xdiag1)}\")\n",
    "xdiag1.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "xdiag1.drop_duplicates(subset=['NodeID', 'Date'], inplace=True)\n",
    "print(f\"# removing duplicates: {len(xdiag1)}\")\n",
    "xdiag1.reset_index(inplace=True, drop=True)\n",
    "xdiag1.groupby(\"NodeID\").agg({\"Date\": [min, max, \"count\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdiag2 = pd.read_csv(os.path.join(file_path, file2), error_bad_lines=False, parse_dates=['Date'])\n",
    "print(f\"# initial: {len(xdiag2)}\")\n",
    "xdiag2 = xdiag2[xdiag2.Date <= tmax2]\n",
    "print(f\"# removing bad dates: {len(xdiag2)}\")\n",
    "xdiag2.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "xdiag2.drop_duplicates(subset=['NodeID', 'Date'], inplace=True)\n",
    "print(f\"# removing duplicates: {len(xdiag2)}\")\n",
    "xdiag2.reset_index(inplace=True, drop=True)\n",
    "xdiag2.groupby(\"NodeID\").agg({\"Date\": [min, max, \"count\"]})['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data\n",
    "xr = pd.concat([xdiag2, xdiag1])\n",
    "xr.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "print(f'Size is {len(xr)}')\n",
    "xr.drop_duplicates(subset=['NodeID', 'Date'], inplace=True)\n",
    "print(f'Size is {len(xr)}')\n",
    "xr.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns with only null vales\n",
    "xr_null = xr.isnull().sum(axis=0)/len(xr)\n",
    "xr_null[xr_null == 1].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_cols_drop = [\n",
    "    'PumpCond1',\n",
    "    'PumpCond2',\n",
    "    'MonthlyElecCost',\n",
    "    'MinEnergyElecBO',\n",
    "    'MinTorqueElecBO',\n",
    "    'CurrentElecBO',\n",
    "    'AvgDHDSLoad',\n",
    "    'AvgDHUSLoad',\n",
    "    'AvgDHDSPOLoad',\n",
    "    'AvgDHUSPOLoad',\n",
    "    'DownholeAnalysisLocale',\n",
    "    'RodAnalysisLocale',\n",
    "    'SurfaceAnalysisLocale',\n",
    "    'InputAnalysisLocale'\n",
    "]\n",
    "xr.drop(columns = xr_cols_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xr.sort_values(by=['NodeID', 'Date'], inplace=True)\n",
    "xr.reset_index(inplace=True, drop=True)\n",
    "xr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddData.add_data(df=xr, db='oasis-dev', table='xdiagresults', schema='stream', merge_type='replace', card_col=None, index_col='Date') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Card Data\n",
    "file_path = r\"C:\\Users\\rai_v\\OneDrive\\Python Coursera\\oasis\\data\\back\\WellTests.E2E.20200728.1116.csv\"\n",
    "well_test = pd.read_csv(file_path, parse_dates=['TestDate'])\n",
    "display(well_test.isnull().sum(axis=0)/well_test.shape[0] * 100)\n",
    "display(well_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
