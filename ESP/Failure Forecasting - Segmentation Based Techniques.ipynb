{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms to be tested which use segmentation based techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from library import lib_aws, esp, preprocess, visualization, metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opttions\n",
    "pd.set_option('display.max_rows', 500)\n",
    "mpl.rcParams['figure.figsize'] = (25,5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "plt.style.use('dark_background')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Data\n",
    "- Import failure_df\n",
    "    - Failures Not to use: `['Unknown', 'Stuck Pump', 'Low Production']`\n",
    "- Import main data\n",
    "- Preprocess the data\n",
    "    - Resample\n",
    "    - Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import failures\n",
    "path = r's3://et-oasis/failure-esp/Oasis ESP Failure Analysis.xlsx'\n",
    "failure_df = pd.read_excel(path)\n",
    "\n",
    "# basic cleaning \n",
    "failure_df['WELL_NAME'] = failure_df['WELL_NAME'].apply(preprocess.node_clean)  # clean well names\n",
    "failure_df['Reason For Pull'] = failure_df['Reason For Pull'].fillna('Unknown')  # fill in nan values\n",
    "failure_df = failure_df[['WELL_NAME', 'Install Date', 'Start Date', 'Fail Date', 'Run Life (Days)', 'Reason For Pull']]  # change columns if need be\n",
    "failure_df.rename(columns={'WELL_NAME': 'NodeID'}, inplace=True)  # modify col name\n",
    "failure_df['Reason For Pull'] = failure_df['Reason For Pull'].map({'Grounded downhole':'Grounded Downhole'}).fillna(failure_df['Reason For Pull'])  # clean up typo\n",
    "\n",
    "# Failures to skip\n",
    "fail_drop = ['Unknown', 'Stuck Pump', 'Low Production']\n",
    "failure_df = failure_df[~failure_df['Reason For Pull'].isin(fail_drop)]\n",
    "failure_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "fail_wells = list(failure_df.NodeID.unique())  # List of wells that are present in failure df\n",
    "failure_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import data\n",
    "query = \"\"\"\n",
    "select * from m_esp_data\n",
    "where \"NodeID\" in {}\n",
    "\"\"\".format(tuple(fail_wells))\n",
    "\n",
    "with lib_aws.PostgresRDS(db='esp-data') as engine:\n",
    "    full_data = pd.read_sql(query, engine, parse_dates=['Date'])\n",
    "\n",
    "wells_in_data = full_data.NodeID.unique()\n",
    "full_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview\n",
    "print('List of wells in failures but not in main data\\n',\n",
    "      *(set(fail_wells) - set(wells_in_data)), sep='\\n')\n",
    "\n",
    "\n",
    "\n",
    "# get avg sampling rate for each well\n",
    "data_info = (full_data.groupby('NodeID')\n",
    "                      .agg({'Date': lambda x: np.mean(x.diff()/pd.Timedelta('1 min')),\n",
    "                            'MotorCurrent': 'count'})\n",
    "                      .rename(columns={'Date': 'Avg Sampling in Min', 'MotorCurrent': 'DataPoints'})\n",
    "                      .round(2))\n",
    "\n",
    "print('\\n------\\nAvg Sampling in the entire dataset {:.2f}'.format(data_info['Avg Sampling in Min'].mean()))\n",
    "print('\\n\\nNote: Check data_info dataframe for well specific sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Resample the data: 1 hr\n",
    "- Transfer Labels\n",
    "- Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Resampling\n",
    "# TODO: Check which columns to drop and use for analysis\n",
    "data_resampled = full_data.copy() # create a copy\n",
    "data_resampled.drop(columns=['OutputAmps', 'OutputVolts', 'YVib'], inplace=True) # drop these columns\n",
    "data_resampled.set_index(['NodeID', 'Date'], inplace=True)  # set index\n",
    "data_resampled.dropna(how='all', inplace=True)  # drop all rows where only nans present, will reduce it even further where we drop unnecessary columns\n",
    "data_resampled.reset_index(inplace=True)\n",
    "\n",
    "# Resampling\n",
    "data_resampled = data_resampled.groupby('NodeID').resample('1H', on='Date').mean()\n",
    "data_resampled.reset_index(inplace=True)\n",
    "data_resampled.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transferring Labels**\n",
    "\n",
    "- Pick a `forecasting_delta` for each failure. For now we consider `15 days`. Change this if need be.\n",
    "- Quick Steps in how its done:\n",
    "    - `Start Date` to (`Fail Date` - `forecasting_delta`) --> Label as `Normal`\n",
    "    - (`Fail Date` - `Forecasting_delta`) to `Fail Date` --> Label as `Reason to Pull`\n",
    "    - `Fail Date` to (`Fail Date` + `1 day`)  -->  Label as `Actual + Reason to Pull`\n",
    "    - Label everythng esle as `Drop`\n",
    "- This gives us the a labeled dataset on which we can run our analysis and implement some splitting strategies\n",
    "- Use the library function: `library.esp.label_esp_data()`\n",
    "- Check docstring for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transferring Labels\n",
    "esp.label_esp_data(data_resampled, \n",
    "                   failure_df, forecasting_delta='15 days',\n",
    "                   verbose=0)  # Change to 1 to see the well specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resampled.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot\n",
    "# plot Specific wells\n",
    "visualization.plot_features(df=data_resampled,\n",
    "                      well_name='Kaitlin Federal 5693 41-28B',\n",
    "                      fail_col='Label',\n",
    "                      zero_label = 'Normal',\n",
    "                      feature_cols=['MotorCurrent','PIP', 'PDP', 'MotorTemperature'],\n",
    "                      mov_avg=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Cleanup**\n",
    "- Drop rows with labels\n",
    "    - `Drop` : Dont have info\n",
    "    - `Actual Label` : Building a forecasting model so we dont need these labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_resampled.copy()\n",
    "data = data[~data['Label'].str.contains('Drop')]  # Dropping labels Drop\n",
    "data = data[~data['Label'].str.contains('Actual')]  # Dropping actual failures\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data\n",
    "\n",
    "Library Class: `library.preprocess.Normalization`\n",
    "\n",
    "A very important task will be on identifying how we normalize the data:\n",
    "\n",
    "**Technique 1: Normalize the Entire Dataset**\n",
    "- Normalize the data on the entire dataset. (This will include all wells and all datapoints)\n",
    "- Once the scaler is trained. Save it and use it whenever needed (while in production)\n",
    "- For Dev: `library.preprocess.Normalization.full_scaling()`\n",
    "\n",
    "**Tehnique2: Well Specific Normalization**\n",
    "- Build a custom scaling funcntion in a well specifc basis.\n",
    "- This will save the max/min values for each KPI.\n",
    "- And while scaling, fucntion will pull the correct max/min to scale the data or build a scaler.\n",
    "- In production, use the scaler which is needed for each group.\n",
    "- For Dev: `library.preprocess.Normalization.well_specific()`\n",
    "\n",
    "**Technique3: Using Scaling in a pipeline**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_normalize = ['MotorCurrent', 'Frequency', 'PIP', 'PDP', 'TubingPressure', 'CasingPressure', 'PIT', 'MotorTemperature', 'XVib']  # columns which will be normalized\n",
    "columns_to_keep = ['NodeID', 'Date', 'Label']  # additional columns we need in the dataset\n",
    "\n",
    "# Well Specific Scaler\n",
    "data_scaled_well = preprocess.Normalization.well_specific(dataset=data,\n",
    "                                                         cols_norm=columns_to_normalize,\n",
    "                                                         cols_keep=columns_to_keep)\n",
    "\n",
    "data_scaled_well.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well Full Scaling\n",
    "data_scaled_full, trained_scaler = preprocess.Normalization.full_scaling(dataset=data,\n",
    "                                                                         cols_norm=columns_to_normalize,\n",
    "                                                                         cols_keep=columns_to_keep)\n",
    "\n",
    "data_scaled_full.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Algorithms\n",
    "\n",
    "We will use Tree based algorithms in this dataset.\n",
    "\n",
    "Shuffling:\n",
    "Method1: Sklearn Train-Test-Split\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
